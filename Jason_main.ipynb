{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Jason's to-do list:\n",
    "  #### - April-June 2014, 2016, 2018\n",
    "  #### - Histogram to understand the distribution of the data\n",
    "  #### - Summary Table\n",
    "  ##### - Ways to summarize the hourly wind data: \n",
    "  - Mean wind speed for a county in a given month  \n",
    "  - Max wind speed for a county in a given month \n",
    "  - Number of times we had high wind speed (15+ m2/sec) \n",
    "  #### - ArcPro Map -- Spatial map of which places receive the most wind events and when?\n",
    "\n",
    "### [Project Guidelines](https://docs.google.com/document/d/13_CxrlfxYHFuPd_BJ2xTrvWDS41ie3TAWVYSBK8_nhU/edit?pli=1)\n",
    "\n",
    "### Spatial and temporal analysis of the wind speed and wind event data and its relationship with crop loss (COL data):\n",
    "  #### - Since we know that crop loss is directly proportional to wind speed, we will try to explore the relationship here. \n",
    "  - What types of wind variables have high correlation with crop loss? \n",
    "  - Is it maximum wind speed, number of high wind speed events in a given month, or something else? \n",
    "  #### - By overlaying the wind speed over the crop cover data (raster data), we can see which crops these high wind events generally damage and when?\n",
    "  #### - Since crop loss data is available at a monthly time-scale at county level, we need to aggregate the wind data at the county level. \n",
    "  #### - What kind of aggregation would you need to do to explore its relationship with crop loss? E.g. (just thoughts) the number of medium vs high wind events in a month for a particular county, the number of tornadoes in the county, etc. \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f08c0c81f1b8a21"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Sources\n",
    "[NOAA Local Climate Data (LCD)](https://www.ncei.noaa.gov/maps/lcd/)\n",
    "[NOAA Storm Events (NCEI)](https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/)\n",
    "[Copernicus Climate Data Store (CDS)](https://cds.climate.copernicus.eu/cdsapp#!/home)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc7323bb4c25e55a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc95b71945dc61b1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import regex as re\n",
    "import netCDF4 as nc\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import hvplot.xarray\n",
    "import holoviews as hv \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.ticker as mticker"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "734e8a966804eacb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.resources import INLINE\n",
    "from rasterio.transform import from_origin\n",
    "from rasterstats import zonal_stats\n",
    "from shapely.geometry import LineString\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.colors import Normalize\n",
    "from netCDF4 import Dataset\n",
    "from IPython.display import display\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bda9e537b38cd67",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Set up Bokeh to display plots inline in the notebook.\n",
    "output_notebook(INLINE)\n",
    "hv.extension('bokeh')\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be92c728dd565a1d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Path to the directory\n",
    "directory = 'Data'\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.isdir(directory):\n",
    "    src_dir = directory\n",
    "else:\n",
    "    src_dir = None\n",
    "\n",
    "print(src_dir)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac774d0c0fd0ea3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "who"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b7e2ff848f935e2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Project extents\n",
    "extent_coords = {'min_lat': 36.998665, 'max_lat': 37.734463,\n",
    "                 'min_lon': -95.964735, 'max_lon': -94.616789}\n",
    "\n",
    "# Extent Filter Function that works with the variable 'extents_coords'\n",
    "def filter_dataframe_on_extent(df, extent_coords, lat1, lon1, lat2, lon2):\n",
    "    min_lat, max_lat = extent_coords['min_lat'], extent_coords['max_lat']\n",
    "    min_lon, max_lon = extent_coords['min_lon'], extent_coords['max_lon']\n",
    "    return df[\n",
    "        ((df[lat1] >= min_lat) & (df[lat1] <= max_lat) &\n",
    "         (df[lon1] >= min_lon) & (df[lon1] <= max_lon)) |\n",
    "        ((df[lat2] >= min_lat) & (df[lat2] <= max_lat) &\n",
    "         (df[lon2] >= min_lon) & (df[lon2] <= max_lon))\n",
    "        ]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47f39387e6fe36fd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the county boundary shapefile\n",
    "sixco_fn = os.path.join(src_dir, 'GIS_files/KS_six_co_bo.shp')\n",
    "data = gpd.read_file(sixco_fn)\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9531a2279283a20e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the Storm event data\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96bba2f8dad613d0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # import matplotlib.pyplot as plt\n",
    "# # import cartopy.crs as ccrs\n",
    "# # import cartopy.feature as cf\n",
    "# # import numpy as np\n",
    "# # from netCDF4 import Dataset\n",
    "# \n",
    "# def ncplotter(mrc, file: str, dep_var: str, width=6, height=6,\n",
    "#               ax_title=\"Colorbar\", plt_title=\"Plot Name\", temp=False, upper_bound=1000):\n",
    "#     \"\"\"\n",
    "#     Function to plot a specified variable from a netCDF format file using matplotlib and cartopy libraries.\n",
    "#     The plot's extent is defined by the global 'extent_coords' variable which is set near the beginnning for the main script, so that is can be called on by other functions in the project. \n",
    "#     # Project extents\n",
    "#     extent_coords = {'min_lat': 36.998665, 'max_lat': 37.734463,\n",
    "#                  'min_lon': -95.964735, 'max_lon': -94.616789} \n",
    "#     An optional temperature \n",
    "#     conversion can be applied if the 'temp' argument is set to True. The function filters out values \n",
    "#     above 'upper_bound' and finds the maximum of the remaining values for each (latitude, longitude) pair.\n",
    "# \n",
    "#     Parameters:\n",
    "#     ----------\n",
    "#     mrc : cartopy.crs object\n",
    "#         The Coordinate Reference System in which to plot the data.\n",
    "#     file : str\n",
    "#         Path to the netCDF format file.\n",
    "#     dep_var : str\n",
    "#         Name of the variable from the netCDF file to be plotted.\n",
    "#     width : int, optional\n",
    "#         Width of the plot in inches. Default is 6.\n",
    "#     height : int, optional\n",
    "#         Height of the plot in inches. Default is 6.\n",
    "#     ax_title : str, optional\n",
    "#         The title for the colorbar. Default is \"Colorbar\".\n",
    "#     plt_title : str, optional\n",
    "#         The title for the plot. Default is \"Plot Name\".\n",
    "#     temp : bool, optional\n",
    "#         If True, the function assumes the provided variable is temperature in Kelvin, \n",
    "#         and will convert the temperature to Fahrenheit before plotting. Default is False.\n",
    "#     upper_bound : float, optional\n",
    "#         The maximum allowed value for the 'dep_var' variable. All values above 'upper_bound' \n",
    "#         are filtered out before the maximum of the remaining values is found. Default is 1000.\n",
    "# \n",
    "#     Returns:\n",
    "#     -------\n",
    "#     None: The function does not return any value, it displays the plot diagram.\n",
    "# \n",
    "#     Author: Gloria Hope\n",
    "#     Modified by: Jason Ehlenberger for extents functionality\n",
    "#     \"\"\"\n",
    "# \n",
    "#  # Open NetCDF dataset\n",
    "#     ds = Dataset(file)\n",
    "# \n",
    "#     # Get variable data\n",
    "#     dep = ds[dep_var][:]\n",
    "#     longs = ds['lon'][:]\n",
    "#     lats = ds['lat'][:]\n",
    "# \n",
    "#     if temp:\n",
    "#         dep = (dep - 273.15) * 9 / 5 + 32  # Convert Kelvin to Fahrenheit\n",
    "# \n",
    "#     xs, ys, zs = [], [], []\n",
    "# \n",
    "#     # filter data that have <=1000 values and get its maximum\n",
    "#     for lat in range(len(lats)):\n",
    "#         for lon in range(len(longs)):\n",
    "#             total = dep[:, lat, lon]\n",
    "#             # Apply upper_bound condition and remove NaNs\n",
    "#             total = total[total <= upper_bound]\n",
    "#             if len(total) > 0:\n",
    "#                 maximum = max(total)\n",
    "#                 xs.append(longs[lon])\n",
    "#                 ys.append(lats[lat])\n",
    "#                 zs.append(maximum)\n",
    "# \n",
    "#     fig = plt.figure(figsize=(width, height), dpi=100, facecolor=\"none\")\n",
    "#     ax = fig.add_subplot(1, 1, 1, projection=mrc)\n",
    "#     ax.stock_img()\n",
    "#     ax.coastlines()\n",
    "#     ax.add_feature(cf.STATES)\n",
    "#     ax.set_extent([extent_coords['min_lon'], extent_coords['max_lon'],\n",
    "#                extent_coords['min_lat'], extent_coords['max_lat']], crs=mrc)\n",
    "# \n",
    "#     plt.scatter(xs, ys, c=zs, cmap='rainbow', marker='s')\n",
    "#     cb = plt.colorbar()\n",
    "#     cb.ax.set_title(ax_title)\n",
    "#     plt.title(plt_title)\n",
    "# \n",
    "#     gls = ax.gridlines(draw_labels=True, color='none')\n",
    "#     gls.top_labels = False\n",
    "#     gls.right_labels = False\n",
    "# \n",
    "#     plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adbb038baee9924c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ncplotter(ccrs.Mercator(),\n",
    "#           os.path.join(src_dir, 'Gridmet_WIND/vs_2018.nc'),\n",
    "#           'wind_speed',\n",
    "#           10,\n",
    "#           6,\n",
    "#           'Maximum Daily Wind Speed',\n",
    "#           'Maximum Daily Wind Speed in 2018',\n",
    "#           False,1000)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21c4c7649af7dd43",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import netCDF4 as nc\n",
    "# import numpy as np\n",
    "\n",
    "# Open the netCDF file\n",
    "file = os.path.join(src_dir, 'Gridmet_WIND/wind_v_u_igust_2014.nc')\n",
    "ds = nc.Dataset(file)\n",
    "\n",
    "# Print out all variable names in the netCDF file\n",
    "print(ds.variables.keys())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc89827ea846345d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Let's say you're interested in the variable named 'variable_of_interest'\n",
    "# # You can print out information about this variable like this:\n",
    "# print(ds.variables['time'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f5a2e3ec431da18",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # You can even read the data of variable\n",
    "# data = np.array(ds.variables['time'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac9b6f9b8bc3e3c9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Now inspect the data. This code prints out the shape, min and max values of the array.\n",
    "# # Replace data with zs if you managed to load it\n",
    "# print(data.shape, np.nanmin(data), np.nanmax(data))\n",
    "# \n",
    "# # Close the Dataset\n",
    "# ds.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc6dd3e95dce026a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # import netCDF4 as nc\n",
    "# # import numpy as np\n",
    "# # import matplotlib.pyplot as plt\n",
    "# # import cartopy.crs as ccrs\n",
    "# # import matplotlib.ticker as mticker\n",
    "# # from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "# \n",
    "# \n",
    "# def ncplotws(filename):\n",
    "#     # Open netCDF file\n",
    "#     dataset = nc.Dataset(filename)\n",
    "# \n",
    "#     # Read variables\n",
    "#     lon = dataset.variables['longitude'][:]\n",
    "#     lat = dataset.variables['latitude'][:]\n",
    "#     time = dataset.variables['time'][:]\n",
    "#     u10 = dataset.variables['u10'][:]  # 10m u-component of wind\n",
    "#     v10 = dataset.variables['v10'][:]  # 10m v-component of wind\n",
    "#     i10fg = dataset.variables['i10fg'][:]  # Instantaneous 10m wind gust\n",
    "# \n",
    "#     # Close the dataset\n",
    "#     dataset.close()\n",
    "# \n",
    "#     # Compute wind speed\n",
    "#     wind_speed = np.sqrt(u10 ** 2 + v10 ** 2)\n",
    "# \n",
    "#     # Plot Wind Speed\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "#     plt.contourf(lon, lat, wind_speed[0, :, :], 60, transform=ccrs.PlateCarree())\n",
    "# \n",
    "#     # Add geographic grid\n",
    "#     gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True)\n",
    "#     gl.top_labels = False\n",
    "#     gl.right_labels = False\n",
    "#     gl.xformatter = LONGITUDE_FORMATTER\n",
    "#     gl.yformatter = LATITUDE_FORMATTER\n",
    "# \n",
    "#     ax.coastlines()\n",
    "# \n",
    "#     # Add title\n",
    "#     ax.set_title('Wind Speed')\n",
    "# \n",
    "#     plt.colorbar(label='Wind Speed (m/s)')\n",
    "#     plt.show()\n",
    "# \n",
    "# # Usage:\n",
    "# # plot_wind_speed_from_netcdf('wind_v_u_igust_2014.nc')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d65c8e376d61ca18",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ncplotws(os.path.join(src_dir, 'Gridmet_WIND/wind_v_u_igust_2014.nc'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56b88aabc0e35f50",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # import geopandas as gpd\n",
    "# \n",
    "# \n",
    "# def ncplotws2(filename, shapefile):\n",
    "#     # Open netCDF file\n",
    "#     dataset = nc.Dataset(filename)\n",
    "# \n",
    "#     # Read variables\n",
    "#     lon = dataset.variables['longitude'][:]\n",
    "#     lat = dataset.variables['latitude'][:]\n",
    "#     time = dataset.variables['time'][:]\n",
    "#     u10 = dataset.variables['u10'][:]  # 10m u-component of wind\n",
    "#     v10 = dataset.variables['v10'][:]  # 10m v-component of wind\n",
    "#     i10fg = dataset.variables['i10fg'][:]  # Instantaneous 10m wind gust\n",
    "# \n",
    "#     # Close the dataset\n",
    "#     dataset.close()\n",
    "# \n",
    "#     # Compute wind speed\n",
    "#     wind_speed = np.sqrt(u10 ** 2 + v10 ** 2)\n",
    "# \n",
    "#     # Plot Wind Speed\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "#     plt.contourf(lon, lat, wind_speed[0, :, :], 60, transform=ccrs.PlateCarree())\n",
    "# \n",
    "#     # Add geographic grid\n",
    "#     gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True, color='blue', linewidth=1)\n",
    "#     gl.top_labels = False\n",
    "#     gl.right_labels = False\n",
    "#     gl.xformatter = LONGITUDE_FORMATTER\n",
    "#     gl.yformatter = LATITUDE_FORMATTER\n",
    "# \n",
    "#     ax.coastlines()\n",
    "# \n",
    "#     # Add title\n",
    "#     ax.set_title('Wind Speed')\n",
    "# \n",
    "#     # Overlay the shapefile\n",
    "#     gdf = gpd.read_file(shapefile)\n",
    "#     gdf.plot(ax=ax, facecolor=\"none\", edgecolor='white', linewidth=2)\n",
    "# \n",
    "#     plt.colorbar(label='Wind Speed (m/s)')\n",
    "#     plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f4fa3b2bd6e7ea6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ncplotws2(os.path.join(src_dir, 'Gridmet_WIND/wind_v_u_igust_2014.nc'),sixco_fn)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40bbaae28a0f2e16",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import netCDF4 as nc\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import cartopy.crs as ccrs\n",
    "# import ipywidgets as widgets\n",
    "# import geopandas as gpd\n",
    "# import matplotlib.ticker as mticker\n",
    "# from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "# from scipy.interpolate import griddata\n",
    "\n",
    "\n",
    "def ncplotws3(filename, shapefile):\n",
    "    dataset = nc.Dataset(filename)\n",
    "    lon = dataset.variables['longitude'][:]\n",
    "    lat = dataset.variables['latitude'][:]\n",
    "    time_units = dataset.variables['time'].units\n",
    "    time_cal = dataset.variables['time'].calendar\n",
    "    time = nc.num2date(dataset.variables['time'][:], time_units, time_cal)\n",
    "    u10 = dataset.variables['u10'][:]\n",
    "    v10 = dataset.variables['v10'][:]\n",
    "    i10fg = dataset.variables['i10fg'][:]\n",
    "    dataset.close()\n",
    "\n",
    "    def plot_netcdf_time(time_index):\n",
    "        wind_speed = np.sqrt(u10[time_index, :, :] ** 2 + v10[time_index, :, :] ** 2)\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "        plt.contourf(lon, lat, wind_speed, 60, transform=ccrs.PlateCarree())\n",
    "    \n",
    "        # Due to the possible large size of the data, we take every nth data point to keep the plot from becoming overloaded\n",
    "        n = 1\n",
    "        ax.barbs(lon[::n], lat[::n], u10[time_index, ::n, ::n], v10[time_index, ::n, ::n], length=5, color='white')\n",
    "\n",
    "        gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True, color='blue', linewidth=1)\n",
    "        gl.top_labels = False\n",
    "        gl.right_labels = False\n",
    "        gl.xformatter = LONGITUDE_FORMATTER\n",
    "        gl.yformatter = LATITUDE_FORMATTER\n",
    "        ax.coastlines()\n",
    "        ax.set_title('Wind Speed and direction ' + str(time[time_index]))\n",
    "        gdf = gpd.read_file(shapefile)\n",
    "        gdf.plot(ax=ax, facecolor=\"none\", edgecolor='white', linewidth=2)\n",
    "\n",
    "        plt.colorbar(label='Wind Speed (m/s)')\n",
    "        plt.show()\n",
    "\n",
    "    time_slider = widgets.IntSlider(min=0, max=len(time) - 1, step=1, value=0, description='Time Index:')\n",
    "    play = widgets.Play(min=0, max=len(time) - 1, step=24, value=0, interval=1000)\n",
    "    widgets.jslink((play, 'value'), (time_slider, 'value'))\n",
    "    \n",
    "    plot_func = widgets.interactive_output(plot_netcdf_time, {'time_index': time_slider})\n",
    "    \n",
    "    display(widgets.HBox([play, time_slider]))\n",
    "    display(plot_func)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0c69b4cbd54f6d4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ncplotws3(os.path.join(src_dir, 'Gridmet_WIND/wind_v_u_igust_2014.nc'),sixco_fn)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7cff4045529116b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def ncplotws4(filename, boundary_shapefile, storm_shapefile, events_csv):\n",
    "    dataset = nc.Dataset(filename)\n",
    "    lon = dataset.variables['longitude'][:]\n",
    "    lat = dataset.variables['latitude'][:]\n",
    "    time_units = dataset.variables['time'].units\n",
    "    time_cal = dataset.variables['time'].calendar\n",
    "    time = nc.num2date(dataset.variables['time'][:], time_units, time_cal)\n",
    "    u10 = dataset.variables['u10'][:]\n",
    "    v10 = dataset.variables['v10'][:]\n",
    "    i10fg = dataset.variables['i10fg'][:]\n",
    "    dataset.close()\n",
    "\n",
    "    def plot_netcdf_time(time_index):\n",
    "        wind_speed = np.sqrt(u10[time_index, :, :] ** 2 + v10[time_index, :, :] ** 2)\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "        plt.contourf(lon, lat, wind_speed, 60, transform=ccrs.PlateCarree())\n",
    "\n",
    "        n = 1\n",
    "        ax.barbs(lon[::n], lat[::n], u10[time_index, ::n, ::n], v10[time_index, ::n, ::n], length=5, color='white')\n",
    "\n",
    "        gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True, color='blue', linewidth=1)\n",
    "        gl.top_labels = False\n",
    "        gl.right_labels = False\n",
    "        gl.xformatter = LONGITUDE_FORMATTER\n",
    "        gl.yformatter = LATITUDE_FORMATTER\n",
    "        ax.coastlines()\n",
    "\n",
    "        ax.set_title('Wind Speed and Direction ' + str(time[time_index]))\n",
    "\n",
    "        # Load and plot boundary shapefile\n",
    "        boundary_gdf = gpd.read_file(boundary_shapefile)\n",
    "        boundary_gdf.plot(ax=ax, facecolor=\"none\", edgecolor='white', linewidth=2.0)\n",
    "\n",
    "        # Load storm events shapefile and CSV, merge them (inner join) and plot\n",
    "        storm_gdf = gpd.read_file(storm_shapefile)\n",
    "        csv_df = pd.read_csv(events_csv, parse_dates=['BEGIN_DATE_TIME_det'])\n",
    "        merged_gdf = storm_gdf.merge(csv_df, how='inner', left_on=\"EVENT_ID\", right_on='EVENT_ID')\n",
    "\n",
    "        for x in range(len(merged_gdf)):\n",
    "            line = merged_gdf.geometry.iloc[x]\n",
    "            ax.plot(line.x, line.y, color='green', linewidth=0.8)\n",
    "            if x < 10:  # Adjust this threshold as needed\n",
    "                ax.text(line.x[0], line.y[0], ' Event ' + str(merged_gdf.EVENT_ID.iloc[x]))\n",
    "\n",
    "        plt.colorbar(label='Wind Speed (m/s)')\n",
    "        plt.show()\n",
    "\n",
    "    time_slider = widgets.IntSlider(min=0, max=len(time) - 1, step=1, value=0, description='Time Index:')\n",
    "    play = widgets.Play(min=0, max=len(time) - 1, step=24, value=0, interval=1000)\n",
    "    widgets.jslink((play, 'value'), (time_slider, 'value'))\n",
    "\n",
    "    plot_func = widgets.interactive_output(plot_netcdf_time, {'time_index': time_slider})\n",
    "\n",
    "    display(widgets.HBox([play, time_slider]))\n",
    "    display(plot_func)\n",
    "\n",
    "\n",
    "ncplotws4(os.path.join(src_dir, 'Gridmet_WIND/wind_v_u_igust_2014.nc'), 'sixco_fn', 'Data/Storm_event/StormEvents2014.shp',\n",
    "          'Data/Storm_event/StormEvents_2014.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3ac8dc36baec63e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ncplotws4(os.path.join(src_dir, 'Gridmet_WIND/wind_v_u_igust_2014.nc'), 'sixco_fn', 'Data/Storm_event/StormEvents2014.shp',\n",
    "          'Data/Storm_event/StormEvents_2014.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb6f9fc1b856776d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   EVENT_ID  EPISODE_ID  BEGIN_YEARMONTH_det  BEGIN_DAY_det  BEGIN_TIME_det  \\\n0    543659       90610               201410              9            2230   \n1    543659       90610               201410              9            2230   \n2    543659       90610               201410              9            2230   \n3    543659       90610               201410              9            2230   \n4    533649       88474               201409              5            1755   \n\n   END_YEARMONTH_det  END_DAY_det  END_TIME_det STATE_det  STATE_FIPS_det  \\\n0             201410           10          1400    KANSAS              20   \n1             201410           10          1400    KANSAS              20   \n2             201410           10          1400    KANSAS              20   \n3             201410           10          1400    KANSAS              20   \n4             201409            5          1755    KANSAS              20   \n\n   ...  DATA_SOURCE_det YEARMONTH_loc LOCATION_INDEX_loc RANGE_loc  \\\n0  ...              CSV        201410                  1      1.41   \n1  ...              CSV        201410                  2      2.94   \n2  ...              CSV        201410                  3      3.27   \n3  ...              CSV        201410                  4      2.90   \n4  ...              CSV        201409                  1      3.83   \n\n   AZIMUTH_loc LOCATION_loc LATITUDE LONGITUDE      LAT2      LON2  \n0          WNW    MOUND VLY  37.2063  -95.4444  37.12378 -95.26664  \n1          ENE    MOUND VLY  37.2102  -95.3682  37.12612 -95.22092  \n2           SE    MOUND VLY  37.1699  -95.3740  37.10194 -95.22440  \n3           SW    MOUND VLY  37.1699  -95.4568  37.10194 -95.27408  \n4          SSW     KNIVETON  37.2700  -94.7000  37.16200 -94.42000  \n\n[5 rows x 60 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>EVENT_ID</th>\n      <th>EPISODE_ID</th>\n      <th>BEGIN_YEARMONTH_det</th>\n      <th>BEGIN_DAY_det</th>\n      <th>BEGIN_TIME_det</th>\n      <th>END_YEARMONTH_det</th>\n      <th>END_DAY_det</th>\n      <th>END_TIME_det</th>\n      <th>STATE_det</th>\n      <th>STATE_FIPS_det</th>\n      <th>...</th>\n      <th>DATA_SOURCE_det</th>\n      <th>YEARMONTH_loc</th>\n      <th>LOCATION_INDEX_loc</th>\n      <th>RANGE_loc</th>\n      <th>AZIMUTH_loc</th>\n      <th>LOCATION_loc</th>\n      <th>LATITUDE</th>\n      <th>LONGITUDE</th>\n      <th>LAT2</th>\n      <th>LON2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>543659</td>\n      <td>90610</td>\n      <td>201410</td>\n      <td>9</td>\n      <td>2230</td>\n      <td>201410</td>\n      <td>10</td>\n      <td>1400</td>\n      <td>KANSAS</td>\n      <td>20</td>\n      <td>...</td>\n      <td>CSV</td>\n      <td>201410</td>\n      <td>1</td>\n      <td>1.41</td>\n      <td>WNW</td>\n      <td>MOUND VLY</td>\n      <td>37.2063</td>\n      <td>-95.4444</td>\n      <td>37.12378</td>\n      <td>-95.26664</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>543659</td>\n      <td>90610</td>\n      <td>201410</td>\n      <td>9</td>\n      <td>2230</td>\n      <td>201410</td>\n      <td>10</td>\n      <td>1400</td>\n      <td>KANSAS</td>\n      <td>20</td>\n      <td>...</td>\n      <td>CSV</td>\n      <td>201410</td>\n      <td>2</td>\n      <td>2.94</td>\n      <td>ENE</td>\n      <td>MOUND VLY</td>\n      <td>37.2102</td>\n      <td>-95.3682</td>\n      <td>37.12612</td>\n      <td>-95.22092</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>543659</td>\n      <td>90610</td>\n      <td>201410</td>\n      <td>9</td>\n      <td>2230</td>\n      <td>201410</td>\n      <td>10</td>\n      <td>1400</td>\n      <td>KANSAS</td>\n      <td>20</td>\n      <td>...</td>\n      <td>CSV</td>\n      <td>201410</td>\n      <td>3</td>\n      <td>3.27</td>\n      <td>SE</td>\n      <td>MOUND VLY</td>\n      <td>37.1699</td>\n      <td>-95.3740</td>\n      <td>37.10194</td>\n      <td>-95.22440</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>543659</td>\n      <td>90610</td>\n      <td>201410</td>\n      <td>9</td>\n      <td>2230</td>\n      <td>201410</td>\n      <td>10</td>\n      <td>1400</td>\n      <td>KANSAS</td>\n      <td>20</td>\n      <td>...</td>\n      <td>CSV</td>\n      <td>201410</td>\n      <td>4</td>\n      <td>2.90</td>\n      <td>SW</td>\n      <td>MOUND VLY</td>\n      <td>37.1699</td>\n      <td>-95.4568</td>\n      <td>37.10194</td>\n      <td>-95.27408</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>533649</td>\n      <td>88474</td>\n      <td>201409</td>\n      <td>5</td>\n      <td>1755</td>\n      <td>201409</td>\n      <td>5</td>\n      <td>1755</td>\n      <td>KANSAS</td>\n      <td>20</td>\n      <td>...</td>\n      <td>CSV</td>\n      <td>201409</td>\n      <td>1</td>\n      <td>3.83</td>\n      <td>SSW</td>\n      <td>KNIVETON</td>\n      <td>37.2700</td>\n      <td>-94.7000</td>\n      <td>37.16200</td>\n      <td>-94.42000</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 60 columns</p>\n</div>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read shapefile and relate table\n",
    "shapefile = os.path.join(src_dir,'Storm_event/StormEvents2014.shp')\n",
    "relate_table = os.path.join(src_dir, 'Storm_event/StormEvents_2014.csv')\n",
    "gdf = gpd.read_file(shapefile)\n",
    "relate_df = pd.read_csv(relate_table)\n",
    "relate_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T19:08:51.140863Z",
     "start_time": "2024-03-27T19:08:51.097933Z"
    }
   },
   "id": "92d6e20bd65a5fcc",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0, Column Name: EVENT_ID\n",
      "Index: 1, Column Name: EPISODE_ID\n",
      "Index: 2, Column Name: BEGIN_YEARMONTH_det\n",
      "Index: 3, Column Name: BEGIN_DAY_det\n",
      "Index: 4, Column Name: BEGIN_TIME_det\n",
      "Index: 5, Column Name: END_YEARMONTH_det\n",
      "Index: 6, Column Name: END_DAY_det\n",
      "Index: 7, Column Name: END_TIME_det\n",
      "Index: 8, Column Name: STATE_det\n",
      "Index: 9, Column Name: STATE_FIPS_det\n",
      "Index: 10, Column Name: YEAR_det\n",
      "Index: 11, Column Name: MONTH_NAME_det\n",
      "Index: 12, Column Name: EVENT_TYPE_det\n",
      "Index: 13, Column Name: CZ_TYPE_det\n",
      "Index: 14, Column Name: CZ_FIPS_det\n",
      "Index: 15, Column Name: CZ_NAME_det\n",
      "Index: 16, Column Name: WFO_det\n",
      "Index: 17, Column Name: BEGIN_DATE_TIME_det\n",
      "Index: 18, Column Name: CZ_TIMEZONE_det\n",
      "Index: 19, Column Name: END_DATE_TIME_det\n",
      "Index: 20, Column Name: INJURIES_DIRECT_det\n",
      "Index: 21, Column Name: INJURIES_INDIRECT_det\n",
      "Index: 22, Column Name: DEATHS_DIRECT_det\n",
      "Index: 23, Column Name: DEATHS_INDIRECT_det\n",
      "Index: 24, Column Name: DAMAGE_PROPERTY_det\n",
      "Index: 25, Column Name: DAMAGE_CROPS_det\n",
      "Index: 26, Column Name: SOURCE_det\n",
      "Index: 27, Column Name: MAGNITUDE_det\n",
      "Index: 28, Column Name: MAGNITUDE_TYPE_det\n",
      "Index: 29, Column Name: FLOOD_CAUSE_det\n",
      "Index: 30, Column Name: CATEGORY_det\n",
      "Index: 31, Column Name: TOR_F_SCALE_det\n",
      "Index: 32, Column Name: TOR_LENGTH_det\n",
      "Index: 33, Column Name: TOR_WIDTH_det\n",
      "Index: 34, Column Name: TOR_OTHER_WFO_det\n",
      "Index: 35, Column Name: TOR_OTHER_CZ_STATE_det\n",
      "Index: 36, Column Name: TOR_OTHER_CZ_FIPS_det\n",
      "Index: 37, Column Name: TOR_OTHER_CZ_NAME_det\n",
      "Index: 38, Column Name: BEGIN_RANGE_det\n",
      "Index: 39, Column Name: BEGIN_AZIMUTH_det\n",
      "Index: 40, Column Name: BEGIN_LOCATION_det\n",
      "Index: 41, Column Name: END_RANGE_det\n",
      "Index: 42, Column Name: END_AZIMUTH_det\n",
      "Index: 43, Column Name: END_LOCATION_det\n",
      "Index: 44, Column Name: BEGIN_LAT_det\n",
      "Index: 45, Column Name: BEGIN_LON_det\n",
      "Index: 46, Column Name: END_LAT_det\n",
      "Index: 47, Column Name: END_LON_det\n",
      "Index: 48, Column Name: EPISODE_NARRATIVE_det\n",
      "Index: 49, Column Name: EVENT_NARRATIVE_det\n",
      "Index: 50, Column Name: DATA_SOURCE_det\n",
      "Index: 51, Column Name: YEARMONTH_loc\n",
      "Index: 52, Column Name: LOCATION_INDEX_loc\n",
      "Index: 53, Column Name: RANGE_loc\n",
      "Index: 54, Column Name: AZIMUTH_loc\n",
      "Index: 55, Column Name: LOCATION_loc\n",
      "Index: 56, Column Name: LATITUDE\n",
      "Index: 57, Column Name: LONGITUDE\n",
      "Index: 58, Column Name: LAT2\n",
      "Index: 59, Column Name: LON2\n"
     ]
    }
   ],
   "source": [
    "# get column names with their index\n",
    "for i, col_name in enumerate(relate_df.columns):\n",
    "    print(f\"Index: {i}, Column Name: {col_name}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T19:08:55.114573Z",
     "start_time": "2024-03-27T19:08:55.110055Z"
    }
   },
   "id": "fd2b32bb0fcc4ef5",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0, Column Name: EVENT_ID\n",
      "Index: 1, Column Name: EPISODE_ID\n",
      "Index: 2, Column Name: STATE_det\n",
      "Index: 3, Column Name: STATE_FIPS_det\n",
      "Index: 4, Column Name: YEAR_det\n",
      "Index: 5, Column Name: MONTH_NAME_det\n",
      "Index: 6, Column Name: EVENT_TYPE_det\n",
      "Index: 7, Column Name: CZ_NAME_det\n",
      "Index: 8, Column Name: BEGIN_DATE_TIME_det\n",
      "Index: 9, Column Name: CZ_TIMEZONE_det\n",
      "Index: 10, Column Name: END_DATE_TIME_det\n",
      "Index: 11, Column Name: SOURCE_det\n",
      "Index: 12, Column Name: FLOOD_CAUSE_det\n",
      "Index: 13, Column Name: BEGIN_LOCATION_det\n",
      "Index: 14, Column Name: END_LOCATION_det\n",
      "Index: 15, Column Name: BEGIN_LAT_det\n",
      "Index: 16, Column Name: BEGIN_LON_det\n",
      "Index: 17, Column Name: END_LAT_det\n",
      "Index: 18, Column Name: END_LON_det\n",
      "Index: 19, Column Name: EPISODE_NARRATIVE_det\n",
      "Index: 20, Column Name: EVENT_NARRATIVE_det\n",
      "Index: 21, Column Name: LATITUDE\n",
      "Index: 22, Column Name: LONGITUDE\n",
      "Index: 23, Column Name: LAT2\n",
      "Index: 24, Column Name: LON2\n"
     ]
    }
   ],
   "source": [
    "# Drop the unwanted columns \n",
    "if len(relate_df.columns) > 25:\n",
    "    rel_table = relate_df.drop(relate_df.columns[np.r_[2:8, 13:15, 16, 20:26, 27:29, 30:40, 41:43, 50:56]], axis=1)\n",
    "else: rel_table = relate_df\n",
    "\n",
    "# get column names with their index, again\n",
    "for i, col_name in enumerate(rel_table.columns):\n",
    "    print(f\"Index: {i}, Column Name: {col_name}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T19:11:38.536198Z",
     "start_time": "2024-03-27T19:11:38.530641Z"
    }
   },
   "id": "fc64cf793f5b97ae",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "rel_table[\"BEGIN_DATE_TIME_det\"] = pd.to_datetime(rel_table[\"BEGIN_DATE_TIME_det\"], format=\"%d-%b-%y %H:%M:%S\")\n",
    "\n",
    "df = pd.merge(gdf, rel_table, on='EVENT_ID')\n",
    "df.head(1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f878528f82d68cf",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import geopandas as gpd\n",
    "# from datetime import datetime\n",
    "# \n",
    "# # Read shapefile and relate table\n",
    "# shapefile = 'Output/StormEvents2014.shp'\n",
    "# relate_table = os.path.join(src_dir, 'Storm_event/StormEvents_2014.csv')\n",
    "# gdf = gpd.read_file(shapefile)\n",
    "# relate_df = pd.read_csv(relate_table)\n",
    "# relate_df[\"BEGIN_DATE_TIME_det\"] = pd.to_datetime(relate_df[\"BEGIN_DATE_TIME_det\"], format=\"%d-%b-%y %H:%M:%S\")\n",
    "# \n",
    "# df = pd.merge(gdf, relate_df, on='EVENT_ID')\n",
    "# \n",
    "# # Checking if the merge is successful\n",
    "# # print(df.shape)\n",
    "# # print(df.head())\n",
    "# \n",
    "# # Checking if there are any records after the filtering\n",
    "# random_date = datetime.strptime(\"2014-01-01\", \"%Y-%m-%d\").date()  # Example date\n",
    "# df_time_filtered = df[df['BEGIN_DATE_TIME_det'].dt.date == random_date]\n",
    "# # print(df_time_filtered.shape)\n",
    "# # print(df_time_filtered.head())\n",
    "# print(df['BEGIN_DATE_TIME_det'].dt.date.unique())\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c25719ec654a4066"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Call the function with your netCDF file and your shapefile\n",
    "# widgets.interact(plot_slider_ws(os.path.join(src_dir, 'Gridmet_WIND/wind_v_u_igust_2014.nc'),'Output/StormEvents2014.shp')) #, os.path.join(src_dir, 'Storm_event/StormEvents_2014.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "717b45cb0515deb5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "whos"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89e24e95b5e64454",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data munging steps in excel if the next function doesn't work.\n",
    "For storm event csv add to column L this '=(LEFT(J1, 2) & \".\" & MID(J1, 3, LEN(J1)-2))*1', and in column M add '=(-LEFT(K1, 2) & \".\" & MID(K1, 3, LEN(K1)-2))*1'\n",
    "Then copy down\n",
    "Copy columns L and M and paste special 'values only' in L and M\n",
    "delete comlumns J and K\n",
    "change column headers to 'LAT2' and 'LON2'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d8a43f1d5712af"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Data munging function customized for Storm Event csv's from [NOAA Storm Events (NCEI)](https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/)\n",
    "\n",
    "# def modify_csv(filename):\n",
    "#     # Load the CSV data into a pandas DataFrame\n",
    "#     df = pd.read_csv(filename)\n",
    "# \n",
    "#     # Convert 'LAT2' and 'LON2' to string, insert decimal point after second digit, and then to floats:\n",
    "#     if 'LAT2' in df.columns:\n",
    "#         df['LAT2'] = (df['LAT2'].astype(str).str.lstrip('-').apply(lambda x: x[:2] + '.' + x[2:])).astype(float)\n",
    "#     if 'LON2' in df.columns:\n",
    "#         df['LON2'] = (df['LON2'].astype(str).str.lstrip('-').apply(lambda x: '-' + x[:2] + '.' + x[2:])).astype(float)\n",
    "# \n",
    "#     \n",
    "#     # Extract year from filename\n",
    "#     match = re.search(r'_d(\\d{4})', filename)\n",
    "#     if match:\n",
    "#         year = match.group(1)\n",
    "#     else:\n",
    "#         year = 'Unknown'\n",
    "# \n",
    "#     # Construct new filename\n",
    "#     old_filename = os.path.basename(filename)\n",
    "#     \n",
    "#     # Change 'd2014_c20231116' format to '2014'\n",
    "#     year = re.search('d(\\d{4})', old_filename).group(1)\n",
    "#     \n",
    "#     # Change 'StormEvents_locations-ftp_v1.0_d2014_c20231116.csv' format to 'StormEvents_locations'\n",
    "#     new_part = old_filename.split('-')[0]\n",
    "#     \n",
    "#     # Construct new filename, change 'StormEvents_locations' format to 'StormEvents_locations_2014.csv'\n",
    "#     new_filename = os.path.join(os.path.dirname(filename), f'{new_part}_{year}.csv')\n",
    "# \n",
    "#     # Save modified DataFrame back to CSV\n",
    "#     df.to_csv(new_filename, index=False)\n",
    "#\n",
    "#\n",
    "# # Use glob.glob() to get all the files that start with 'Storm_event/StormEvents'\n",
    "# files = glob.glob(os.path.join(src_dir, 'Storm_event/StormEvents_*'))\n",
    "# \n",
    "# # Run the modify_csv function for each file using list comprehension\n",
    "# for f in files:\n",
    "#     modify_csv(f)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9790347a00945ebd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b1b357e7a6c0064"
  },
  {
   "cell_type": "markdown",
   "source": [
    "    event_ids_to_add = [498864, 508406, 508407, 508408, 508409, 508413, 508414, 508440, 508468, 508476, 508480, 508481, 508482, 516243, 516244, 516245, 516246, 526836, 526837, 533868, 542919, 543368, 543376, 543645, 543646, 543667, 627803, 627804, 627806, 630383, 630387, 632580, 633147, 636638, 655474, 659269, 659272, 659273, 659274, 659292, 660117, 660118, 661872, 661873, 662309, 662310, 663504, 663539, 663987, 663989, 663991, 754111, 755412, 756557, 756559, 756560, 756562, 756563, 756564, 756569, 756576, 756578, 756608, 756609, 756611, 756861, 765900, 765905, 772228, 772229, 774573, 774574, 779898, 779899, 780508, 780781, 787624, 787627, 787630, 792763]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5c2026309f31062"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def join_and_save(src_dir, year, extent_coords=None):\n",
    "    details = pd.read_csv(os.path.join(src_dir, f'Storm_event\\StormEvents_details_{year}.csv'))\n",
    "    locations = pd.read_csv(os.path.join(src_dir, f'Storm_event\\StormEvents_locations_{year}.csv')).drop(\n",
    "        columns='EPISODE_ID')\n",
    "\n",
    "    # List of Event IDs to be kept even after extent filtering\n",
    "    event_ids_to_add = [498864, 508406, 508407, 508408, 508409, 508413, 508414, 508440, 508468, 508476, 508480, 508481, 508482, 516243, 516244, 516245, 516246, 526836, 526837, 533868, 542919, 543368, 543376, 543645, 543646, 543667, 627803, 627804, 627806, 630383, 630387, 632580, 633147, 636638, 655474, 659269, 659272, 659273, 659274, 659292, 660117, 660118, 661872, 661873, 662309, 662310, 663504, 663539, 663987, 663989, 663991, 754111, 755412, 756557, 756559, 756560, 756562, 756563, 756564, 756569, 756576, 756578, 756608, 756609, 756611, 756861, 765900, 765905, 772228, 772229, 774573, 774574, 779898, 779899, 780508, 780781, 787624, 787627, 787630, 792763]\n",
    "\n",
    "    # Separate out specific records\n",
    "    details_to_add = details[details['EVENT_ID'].isin(event_ids_to_add)]\n",
    "    locations_to_add = locations[locations['EVENT_ID'].isin(event_ids_to_add)]\n",
    "\n",
    "    # Remove these specific records from details and locations before filtering\n",
    "    details = details[~details['EVENT_ID'].isin(event_ids_to_add)]\n",
    "    locations = locations[~locations['EVENT_ID'].isin(event_ids_to_add)]\n",
    "\n",
    "    if extent_coords:\n",
    "        details = filter_dataframe_on_extent(details, extent_coords, 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON')\n",
    "        locations = filter_dataframe_on_extent(locations, extent_coords, 'LATITUDE', 'LONGITUDE', 'LAT2', 'LON2')\n",
    "\n",
    "        # Append the separated records back to details and locations after filtering\n",
    "        details = pd.concat([details, details_to_add])\n",
    "        locations = pd.concat([locations, locations_to_add])\n",
    "\n",
    "    details.columns = [col + '_det' if col not in ['EVENT_ID', 'EPISODE_ID'] else col for col in details.columns]\n",
    "    locations.columns = [\n",
    "        col + '_loc' if col not in ['EVENT_ID', 'EPISODE_ID', 'LATITUDE', 'LONGITUDE', 'LAT2', 'LON2'] else col for col\n",
    "        in locations.columns]\n",
    "\n",
    "    merged_df = pd.merge(details, locations, on='EVENT_ID', how='outer')\n",
    "\n",
    "    column_order = ['EVENT_ID', 'EPISODE_ID'] + [col for col in merged_df.columns if\n",
    "                                                 col not in ['EVENT_ID', 'EPISODE_ID']]\n",
    "    merged_df = merged_df[column_order]\n",
    "\n",
    "    merged_df.to_csv(os.path.join(src_dir, f'Storm_event\\StormEvents_{year}.csv'), index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21fb8e9d6768eb66"
  },
  {
   "cell_type": "markdown",
   "source": [
    "for year in [2014, 2016, 2018]:\n",
    "    join_and_save(src_dir, year, extent_coords = extent_coords)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8fb7b9c3aac5278"
  },
  {
   "cell_type": "markdown",
   "source": [
    "def convert_dataframe_to_shapefile(source_file, output_file, extent_coords):\n",
    "    \"\"\"\n",
    "    This function reads a CSV file and filters it based on latitude and longitude. \n",
    "    Then it drops NA rows from 'LATITUDE', 'LONGITUDE', 'LAT2', 'LON2' and converts the DataFrame into GeoDataFrame.\n",
    "    Finally, the output is saved into shapefile format.\n",
    "    \n",
    "    Args:\n",
    "    source_file: str: Path of the source CSV file.\n",
    "    output_file: str: Path of the output shapefile.\n",
    "    extent_coords: dict: \n",
    "        A dictionary containing the coordinates for area bounds to the filter. \n",
    "        Keys are 'min_lat', 'max_lat', 'min_lon', 'max_lon', belongs to either lon-lat pair. \n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the CSV data into a pandas DataFrame\n",
    "    df = pd.read_csv(source_file, dtype={'AZIMUTH': str, 'LOCATION': str})\n",
    "\n",
    "    # Pull out bounds for easier reference\n",
    "    min_lat, max_lat = extent_coords['min_lat'], extent_coords['max_lat']\n",
    "    min_lon, max_lon = extent_coords['min_lon'], extent_coords['max_lon']\n",
    "\n",
    "    # Filter the records that either start or end within the given extents\n",
    "    df_filtered = df[\n",
    "        (\n",
    "                (df['LATITUDE'] >= min_lat) &\n",
    "                (df['LATITUDE'] <= max_lat) &\n",
    "                (df['LONGITUDE'] >= min_lon) &\n",
    "                (df['LONGITUDE'] <= max_lon)\n",
    "        ) |\n",
    "        (\n",
    "                (df['LAT2'] >= min_lat) &\n",
    "                (df['LAT2'] <= max_lat) &\n",
    "                (df['LON2'] >= min_lon) &\n",
    "                (df['LON2'] <= max_lon)\n",
    "        )\n",
    "        ]\n",
    "\n",
    "    # Drop NA values from 'LATITUDE', 'LONGITUDE', 'LAT2', 'LON2'\n",
    "    df_filtered = df_filtered.dropna(subset=['LATITUDE', 'LONGITUDE', 'LAT2', 'LON2'])\n",
    "\n",
    "    # Create a new 'geometry' column in the DataFrame that contains LineString objects\n",
    "    df_filtered['geometry'] = df_filtered.apply(lambda row: LineString(\n",
    "        [(row['LONGITUDE'], row['LATITUDE']), (row['LON2'], row['LAT2'])]), axis=1)\n",
    "\n",
    "    # Convert the DataFrame to a GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(df_filtered, geometry='geometry')\n",
    "\n",
    "    # Save the GeoDataFrame as a shapefile\n",
    "    gdf.to_file(output_file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c1f43a842dfc624"
  },
  {
   "cell_type": "markdown",
   "source": [
    "source_file = os.path.join(src_dir, 'Storm_event/StormEvents2014.csv')\n",
    "output_file = 'Output/StormEvents2014.shp'\n",
    "extent_coords = {'min_lat': 36.998665, 'max_lat': 37.734463,\n",
    "                 'min_lon': -95.964735, 'max_lon': -94.616789}\n",
    "\n",
    "convert_dataframe_to_shapefile(source_file, output_file, extent_coords)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb8d82eb671fe927"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Exploratory Analysis\n",
    "1) Describe the Data\n",
    "2) Trends\n",
    "3) Summary tables\n",
    "4) drop unneeded columns\n",
    "5) drop duplicates\n",
    "6) drop outliers\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "554985b5bcce07c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "windbreaks",
   "language": "python",
   "display_name": "WindBreaks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
