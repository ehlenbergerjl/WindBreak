{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "554985b5bcce07c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# EXPLORATORY DATA ANALYSIS\n",
    "1) Describe the Data\n",
    "2) Trends\n",
    "3) Summary tables\n",
    "4) drop unneeded columns\n",
    "5) drop duplicates\n",
    "6) drop outliers\n",
    " \n",
    "## Update from Bhuwan & possible tasks for next week and beyond: (3/27/2024)\n",
    "### * Task 1 - see if you can create a summary statistics using this data and find any relationship between the storm events and crop damages \n",
    "  - I have added storm event line shape file. It has details about the storm intensity, etc. \n",
    "  - The file is called “stormevents_line_details.shp” and its in [this folder](https://drive.google.com/drive/u/1/folders/1EWWnjXzVrVY3GUTp3hwoWEh5UsmP2E_E). \n",
    "  - Maybe we can create a smaller size buffer. I used 5 mile buffer of the line and I think it was too big. \n",
    "  - You will have to aggregate the storm events data at county-level to compare it with crop loss because crop loss is at the county level only. \n",
    "\n",
    "## Update from Bhuwan & possible tasks for next week and beyond: (3/30/2024)\n",
    "### * Task 2 - Mean monthly wind speed (netcdf data)\n",
    "  - Considering the time constraints, let’s use the mean monthly wind speed data from [this folder](https://drive.google.com/drive/folders/1zxvOB6XwXkiOWh1QhQyzTEd5VrL87Puw?usp=drive_link).\n",
    "  - The data source is [climatologylab.org](https://www.climatologylab.org/gridmet.html) \n",
    "  - Pick any one year and do the analysis, (1) spatial and temporal trends of wind speed in the study area, (2) its relationship with crop loss (correlation coefficient).  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c458e13e7c1e40f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Getting started\n",
    "Intstall the latest [MiniConda](https://docs.anaconda.com/free/miniconda/) for your Operating System \n",
    "- When asked, I recommend you install for 'Just me', not 'All users'\n",
    "- This is important if you are not admin on your system\n",
    "\n",
    "Create your environment in a Python shell (Terminal in PyCharm)\n",
    "- Replace 'myenv' with whatever name you want to give your new environment\n",
    "- I'm calling mine 'WindBreaks'\n",
    "Install these packages with the '--yes' option to keep it moving without asking questions \n",
    "- ... or you can leave that out if you want to know what is being upgraded/downgraded/installed for compatibility and dependency\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "    # Create new env\n",
    "    conda create --name myenv\n",
    "    # Activate new env \n",
    "    conda activate myenv\n",
    "    # Install the packages\n",
    "    python -m pip install jupyter\n",
    "    conda install conda-content-trust --yes\n",
    "    conda install -c anaconda ipykernel --yes\n",
    "    conda install -c conda-forge IPython --yes\n",
    "    conda install -c conda-forge netCDF4 --yes\n",
    "    conda install -c conda-forge rasterio --yes\n",
    "    conda install -c conda-forge numpy --yes\n",
    "    conda install -c conda-forge xarray --yes\n",
    "    conda install -c pyviz hvplot --yes\n",
    "    conda install -c conda-forge holoviews --yes\n",
    "    conda install -c anaconda pandas --yes\n",
    "    conda install -c conda-forge geopandas --yes\n",
    "    conda install -c conda-forge rasterstats --yes\n",
    "    conda install -c anaconda seaborn --yes\n",
    "    conda install -c conda-forge matplotlib --yes\n",
    "    conda install -c anaconda regex --yes\n",
    "    conda install -c conda-forge cartopy --yes\n",
    "    conda install -c conda-forge ipywidgets --yes\n",
    "\n",
    "    # Create the Jupyter Kernal for your notebook\n",
    "    python -m ipykernel install --user --name WindBreaks --display-name \"WindBreaks\"\n",
    "\n",
    "    \n",
    "## Possible additional resources:\n",
    "### Tutorials\n",
    "   #### Exploratory Analysis\n",
    "   - [Tutorial 1](https://www.geeksforgeeks.org/quick-guide-to-exploratory-data-analysis-using-jupyter-notebook/)\n",
    "   - [YOUR Data Teacher (YouTube video)](https://www.youtube.com/watch?v=iZ2MwVWKwr4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f08c0c81f1b8a21",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Project Notes:\n",
    "## Standardization:\n",
    "### Colors\n",
    "  - April red hex E41A1C\n",
    "  - May blue hex 377EB8\n",
    "  - June green hex 4DAF4A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7323bb4c25e55a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Sources:\n",
    "- [NOAA Local Climate Data (LCD)](https://www.ncei.noaa.gov/maps/lcd/)\n",
    "- [NOAA Storm Events (NCEI)](https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/)\n",
    "- [Copernicus Climate Data Store (CDS)](https://cds.climate.copernicus.eu/cdsapp#!/home)\n",
    "- [Climatology Lab](https://www.climatologylab.org/gridmet.html) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc1f96ad46d24f5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Prepare the EDA Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc95b71945dc61b1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734e8a966804eacb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import regex as re\n",
    "import netCDF4 as nc\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import hvplot.xarray\n",
    "import holoviews as hv \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.ticker as mticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bda9e537b38cd67",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.resources import INLINE\n",
    "from rasterio.transform import from_origin\n",
    "from rasterstats import zonal_stats\n",
    "from shapely.geometry import LineString\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.colors import Normalize\n",
    "from netCDF4 import Dataset\n",
    "from pyproj import CRS\n",
    "from IPython.display import display\n",
    "import chardet\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be92c728dd565a1d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up Bokeh to display plots inline in the notebook.\n",
    "output_notebook(INLINE)\n",
    "hv.extension('bokeh')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2769cacb0a0693a5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Set data source variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac774d0c0fd0ea3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Path to the directory\n",
    "directory = 'Data'\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.isdir(directory):\n",
    "    src_dir = directory\n",
    "else:\n",
    "    src_dir = None\n",
    "\n",
    "print(src_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d373e296bf7ba0a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Set the extents of the Area of Interest (AOI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f39387e6fe36fd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Project extents\n",
    "extent_coords = {'min_lat': 36.998665, 'max_lat': 37.734463,\n",
    "                 'min_lon': -95.964735, 'max_lon': -94.616789}"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Us Jupyter magik to list all variables and function loaded in the interactive workspace\n",
    "%whos"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b7e2ff848f935e2",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a41e4df259393655",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Define Functions for EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90011424038a70c2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## - Extent Filter Function that works with the variable 'extents_coords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa66901c8d3fcf8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_dataframe_on_extent(df, extent_coords, lat1, lon1, lat2, lon2):\n",
    "    min_lat, max_lat = extent_coords['min_lat'], extent_coords['max_lat']\n",
    "    min_lon, max_lon = extent_coords['min_lon'], extent_coords['max_lon']\n",
    "    return df[\n",
    "        ((df[lat1] >= min_lat) & (df[lat1] <= max_lat) &\n",
    "         (df[lon1] >= min_lon) & (df[lon1] <= max_lon)) |\n",
    "        ((df[lat2] >= min_lat) & (df[lat2] <= max_lat) &\n",
    "         (df[lon2] >= min_lon) & (df[lon2] <= max_lon))\n",
    "        ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fd9979c98a34bd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## - Convert Geodataframe to a Shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dd5db3e4dbf7cc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_geodataframe_to_shapefile(gdf, output_file, extent_coords):\n",
    "    \"\"\"\n",
    "    This function reads a GeoDataFrame, and filters it based on latitude and longitude. \n",
    "    It then drops NA rows from 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON' and saves the resultant GeoDataFrame into shapefile format.   \n",
    "    Args:\n",
    "    df: GeoDataFrame: The input GeoDataFrame.\n",
    "    output_file: str: Path of the output shapefile.\n",
    "    extent_coords: dict: \n",
    "        A dictionary containing the coordinates for area bounds to the filter. \n",
    "        Keys are 'min_lat', 'max_lat', 'min_lon', 'max_lon', belongs to either lat-lon pair. \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter the records that either start or end within the given extents\n",
    "    gdf_filtered = filter_dataframe_on_extent(gdf, extent_coords, 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON')\n",
    "\n",
    "    # Save the GeoDataFrame as a shapefile\n",
    "    gdf_filtered.to_file(output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567e0816b2379b41",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## - Create Buffer Shapefile from Geodataframe"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_buffer(gdf, distance, output_path):\n",
    "    \"\"\"\n",
    "    Creates a buffer around the geometries in a given GeoDataFrame and saves the output to a shapefile.\n",
    "\n",
    "    Args:\n",
    "    gdf: A GeoDataFrame containing the geometries to buffer\n",
    "    distance: The distance to buffer around each geometry. This can be a numeric value or a string. \n",
    "        If a numeric value, the units should be in miles for CRS in State Plane or kilometers for all others. \n",
    "        If a string, it should end with 'm' for meters or 'ft' for feet.\n",
    "    output_path: The path to the output shapefile\n",
    "\n",
    "    Returns:\n",
    "    A new GeoDataFrame with buffered geometries in the original CRS. Also writes the new GeoDataFrame to a shapefile.\n",
    "    \"\"\"\n",
    "    original_crs = gdf.crs\n",
    "    crs_unit = original_crs.axis_info[0].unit_name.lower()\n",
    "\n",
    "    buffer_distance = distance\n",
    "    if isinstance(distance, str):\n",
    "        if distance.endswith('m'):\n",
    "            buffer_distance = float(distance.rstrip('m'))\n",
    "            if crs_unit == 'us survey foot' or crs_unit == 'foot':\n",
    "                buffer_distance *= 3.281\n",
    "        elif distance.endswith('ft'):\n",
    "            buffer_distance = float(distance.rstrip('ft'))\n",
    "            if crs_unit == 'meter':\n",
    "                buffer_distance /= 3.281\n",
    "    else:\n",
    "        if crs_unit == 'degree':\n",
    "            gdf = gdf.to_crs(epsg=3395)\n",
    "            crs_unit = gdf.crs.axis_info[0].unit_name.lower()\n",
    "            buffer_distance = distance * 1000\n",
    "        elif crs_unit == 'us survey foot':\n",
    "            buffer_distance = distance * 5280.01016\n",
    "        elif crs_unit == 'foot':\n",
    "            buffer_distance = distance * 5280\n",
    "        elif crs_unit == 'meter':\n",
    "            buffer_distance = distance * 1000\n",
    "\n",
    "    gdf['geometry'] = gdf.geometry.buffer(buffer_distance)\n",
    "\n",
    "    if original_crs != gdf.crs:\n",
    "        gdf = gdf.to_crs(original_crs)\n",
    "\n",
    "    gdf.to_file(output_path)\n",
    "\n",
    "    return gdf"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cb81d703a194a18",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c4b80eb8d2f50f2c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## - Data munging function customized for Storm Event csv's from NCEI"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO: BROKEN, but I need to sleep. takle tomorrow\n",
    "\n",
    "# def modify_csv(filename: str):\n",
    "#     \"\"\"\n",
    "#     This function modifies the content of a CSV file from the Storm Events dataset.\n",
    "# \n",
    "#     Parameters:\n",
    "#     filename (str): The full path of the original CSV file to be modified.\n",
    "# \n",
    "#     Returns:\n",
    "#     None\n",
    "#     \"\"\"\n",
    "# \n",
    "#     # Extract base name from filename without year and extension\n",
    "#     base_name = os.path.splitext(os.path.basename(filename))[0].rsplit('_', 1)[0]\n",
    "# \n",
    "#     # Skip processing if the file has already been processed\n",
    "#     if base_name in existing_files:\n",
    "#         print(f\"{base_name} already exists. Skipping...\")\n",
    "#         return\n",
    "# \n",
    "#     # If the file is a 'locations' file, check if the corresponding 'details' file has been processed\n",
    "#     if \"locations\" in filename and base_name.replace('locations', 'details') not in existing_files:\n",
    "#         print(f\"The corresponding details file for {base_name} has not been processed yet. Skipping...\")\n",
    "#         return\n",
    "# \n",
    "#     # Read the file using detected encoding\n",
    "#     rawdata = open(filename, 'rb').read()\n",
    "#     result = chardet.detect(rawdata)\n",
    "#     encoding = result['encoding']\n",
    "#     df = pd.read_csv(filename, encoding=encoding)\n",
    "# \n",
    "#     # Modify the latitudes and longitudes in the dataframe\n",
    "#     if 'LAT2' in df.columns:\n",
    "#         df['LAT2'] = df['LAT2'].astype(str).str.lstrip('-').apply(\n",
    "#             lambda x: x[:2] + '.' + x[2:].replace('.', '')).astype(float)\n",
    "# \n",
    "#     if 'LON2' in df.columns:\n",
    "#         df['LON2'] = df['LON2'].astype(str).str.lstrip('-').apply(\n",
    "#             lambda x: '-' + x[:2] + '.' + x[2:].replace('.', '')).astype(float)\n",
    "# \n",
    "#     # Prepare a new filename\n",
    "#     new_filename = os.path.join(os.path.dirname(filename), f'{base_name}.csv')\n",
    "# \n",
    "#     # Save the modified dataframe into the new file\n",
    "#     df.to_csv(new_filename, index=False)\n",
    "# \n",
    "#     print(f\"File {base_name} has been processed and saved as {new_filename}.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "918d59b7dc34d890",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e5c2026309f31062",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## - Join Storm Event Tables"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO: BROKEN, but I need to sleep. takle tomorrow\n",
    "\n",
    "# def join_and_save(src_dir, year, event_id_exceptions=[], extent_coords=None):\n",
    "#     # Define all matching CSV files\n",
    "#     all_files = glob.glob(os.path.join(src_dir, 'Storm_event/StormEvents_*-ftp*d*_c*.csv'))\n",
    "#     # Get a list of existing files without the directory and extension\n",
    "#     existing_files = [os.path.splitext(os.path.basename(f))[0] for f in all_files]\n",
    "# \n",
    "#     # Check if a file for this year's details already exists\n",
    "#     if any(f'StormEvents_details_{year}' in filename for filename in existing_files):\n",
    "#         print(f\"File for details of year {year} already exists.\")\n",
    "#         details = pd.read_csv(os.path.join(src_dir, f'Storm_event\\\\StormEvents_details_{year}.csv'))\n",
    "#     else:\n",
    "#         print(f\"Unable to find details file for year {year}.\")\n",
    "#         return\n",
    "# \n",
    "#     # Only process 'locations' file if 'details' file already exists and 'locations' does not\n",
    "#     if any(f'StormEvents_locations_{year}' in filename for filename in existing_files):\n",
    "#         print(f'File for locations of year {year} already exists. Skipping...')\n",
    "#         return\n",
    "#     else:\n",
    "#         print(f'Processing locations file for year {year}.')\n",
    "#         locations = pd.read_csv(os.path.join(src_dir, f'Storm_event\\\\StormEvents_locations_{year}.csv')).drop(\n",
    "#             columns='EPISODE_ID')\n",
    "# \n",
    "#     # Separate out specific records\n",
    "#     details_to_add = details[details['EVENT_ID'].isin(event_id_exceptions)]\n",
    "#     locations_to_add = locations[locations['EVENT_ID'].isin(event_id_exceptions)]\n",
    "# \n",
    "#     # Remove these specific records from details and locations before filtering\n",
    "#     details = details[~details['EVENT_ID'].isin(event_id_exceptions)]\n",
    "#     locations = locations[~locations['EVENT_ID'].isin(event_id_exceptions)]\n",
    "# \n",
    "#     if extent_coords:\n",
    "#         details = filter_dataframe_on_extent(details, extent_coords, 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON')\n",
    "#         locations = filter_dataframe_on_extent(locations, extent_coords, 'LATITUDE', 'LONGITUDE', 'LAT2', 'LON2')\n",
    "# \n",
    "#         # Append the separated records back to details and locations after filtering\n",
    "#         details = pd.concat([details, details_to_add])\n",
    "#         locations = pd.concat([locations, locations_to_add])\n",
    "# \n",
    "#     details.columns = [col + '_det' if col not in ['EVENT_ID', 'EPISODE_ID'] else col for col in details.columns]\n",
    "#     locations.columns = [\n",
    "#         col + '_loc' if col not in ['EVENT_ID', 'EPISODE_ID', 'LATITUDE', 'LONGITUDE', 'LAT2', 'LON2'] else col for col\n",
    "#         in locations.columns]\n",
    "# \n",
    "#     merged_df = pd.merge(details, locations, on='EVENT_ID', how='outer')\n",
    "# \n",
    "#     column_order = ['EVENT_ID', 'EPISODE_ID'] + [col for col in merged_df.columns if\n",
    "#                                                  col not in ['EVENT_ID', 'EPISODE_ID']]\n",
    "#     merged_df = merged_df[column_order]\n",
    "# \n",
    "#     merged_df.to_csv(os.path.join(src_dir, f'Storm_event\\\\StormEvents_{year}.csv'), index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfa9efdc4e4cd2a9",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load basemaps and boundary files for AOI"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a05b96ef2e81ad02"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the county boundary shapefile\n",
    "sixco_fn = os.path.join(src_dir, 'GIS_files/KS_six_co_bo.shp')\n",
    "sixco_data = gpd.read_file(sixco_fn)\n",
    "\n",
    "# Get the CRS\n",
    "crs = CRS(sixco_data.crs)\n",
    "\n",
    "# Name of the CRS\n",
    "print(\"Name:\", crs.name)\n",
    "# EPSG of the CRS\n",
    "if crs.to_epsg():\n",
    "    print('EPSG:', crs.to_epsg())\n",
    "else:\n",
    "    print('No EPSG found for this CRS')\n",
    "# Unit of the CRS\n",
    "print(\"Unit:\", crs.axis_info[0].unit_name)\n",
    "\n",
    "# Preview Data\n",
    "sixco_data.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9531a2279283a20e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sixco_data = sixco_data.to_crs(\"EPSG:6469\")\n",
    "sixco_data.crs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d063ca6db7fa6f36",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5e9a49ffc18b1c42",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Examine Storm Event Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c7f24344b1ccfc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## - Examine Storm Event Shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a37b32cc917fb9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the Storm event data\n",
    "input_shp_file = os.path.join(src_dir, 'Storm_event/stormevents_line_details.shp')\n",
    "\n",
    "# Load original data\n",
    "original_data = gpd.read_file(input_shp_file)\n",
    "\n",
    "# Preview Data\n",
    "original_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0b49da1b91c4b1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get column names with their index\n",
    "for i, col_name in enumerate(original_data.columns):\n",
    "    print(f\"Index: {i}, Column Name: {col_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce323fed30540b9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modify data as needed\n",
    "# # Drop duplicates and redunant data\n",
    "# # Shorten column names to to Max ten character column name for shapefiles\n",
    "# # Standardize related data\n",
    "modified_data = original_data.drop(columns='EVENT_ID_1').rename(columns={'BEGIN_YEAR': 'BEGIN_YRMO', 'END_YEARMO': 'END_YRMO'}) \n",
    "\n",
    "# Check your modifications\n",
    "for i, col_name in enumerate(modified_data.columns):\n",
    "    print(f\"Index: {i}, Column Name: {col_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bba2f8dad613d0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the CRS\n",
    "crs = CRS(modified_data.crs)\n",
    "\n",
    "# Name of the CRS\n",
    "print(\"Name:\", crs.name)\n",
    "# EPSG of the CRS\n",
    "if crs.to_epsg():\n",
    "    print('EPSG:', crs.to_epsg())\n",
    "else:\n",
    "    print('No EPSG found for this CRS')\n",
    "# Unit of the CRS\n",
    "print(\"Unit:\", crs.axis_info[0].unit_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d33d948f233a413",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reproject / transform to state plane\n",
    "modified_data = modified_data.to_crs(\"EPSG:6469\")\n",
    "# Get the CRS\n",
    "crs = CRS(modified_data.crs)\n",
    "\n",
    "# Name of the CRS\n",
    "print(\"Name:\", crs.name)\n",
    "# EPSG of the CRS\n",
    "if crs.to_epsg():\n",
    "    print('EPSG:', crs.to_epsg())\n",
    "else:\n",
    "    print('No EPSG found for this CRS')\n",
    "# Unit of the CRS\n",
    "print(\"Unit:\", crs.axis_info[0].unit_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7195bcfa9899cb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save modified data as a new Shapefile\n",
    "modified_shp_file = os.path.join(src_dir, 'Storm_event/modified_storm_events.shp')\n",
    "modified_data.to_file(modified_shp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a727dad8168876ec",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter the GeoDataFrame based on extent and save as a new shapefile\n",
    "output_shp_file = os.path.join(src_dir, 'Storm_event/filtered_storm_events.shp')\n",
    "convert_geodataframe_to_shapefile(modified_data, output_shp_file, extent_coords)\n",
    "\n",
    "# Load the filtered data\n",
    "se_gdf_filtered = gpd.read_file(output_shp_file)\n",
    "se_gdf_filtered.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2963b86dbefdc3ca",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Specify the path to the input geodataframe\n",
    "input_gdf = se_gdf_filtered\n",
    "# Specify the buffer distance as a float or int\n",
    "buf_dist = 3\n",
    "# or you can specify a str if you wamt to buffer by meters or feet instesd of kilometers and and miles, respectively \n",
    "# buf_dist = '3000ft'\n",
    "buf_dist_str = str(buf_dist).replace('.', '_')\n",
    "# Specify the path to the output shapefile\n",
    "output_path = os.path.join(src_dir, f'Storm_event/storm_line_{buf_dist_str}_buf.shp')\n",
    "\n",
    "# Create a buffered GeoDataFrame\n",
    "# Note: For Geographic Coordinate Systems, provide buffer distance in kilometers. \n",
    "# For State Plane Coordinate Systems, provide buffer distance in miles.\n",
    "buffered_storm_events = create_buffer(input_gdf, buf_dist, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f0ad81022b4994",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Group by 'county' and 'END_YEARMO' and aggregate\n",
    "se_gdf = se_gdf_filtered #.groupby(['county', 'END_YEARMO']).sum().reset_index()\n",
    "\n",
    "min_value = se_gdf['BEGIN_YRMO'].min()\n",
    "max_value = se_gdf['END_YRMO'].max()\n",
    "\n",
    "print('Minimum value in BEGIN_YRMO column:', min_value)\n",
    "print('Maximum value in END_YRMO column:', max_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79e779d02dcef00",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## - Create Wind Event Relate Tables (under construction)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO: BROKEN, but I need to sleep. takle tomorrow\n",
    "\n",
    "# # HERE ARE THREE OPTIONS FOR CALLING THE STROM EVENT DATA MUNGE!! PLEASE COMMENT OUT ALL BUT ONE METHOD!!\n",
    "# \n",
    "# # Keep this part active....\n",
    "# # Define all matching CSV files\n",
    "# all_files = glob.glob(os.path.join(src_dir, 'Storm_event/StormEvents_*-ftp*d*_c*.csv'))\n",
    "# # Get a list of existing files without the directory and extension\n",
    "# existing_files = [os.path.splitext(os.path.basename(f))[0] for f in all_files]\n",
    "# \n",
    "# Choose 1), 2), or 3). Comment the two unused options out.\n",
    "# # 1) Define year specific files\n",
    "# years = [2014, 2016, 2018]  # or any other specific years\n",
    "# for year in years:\n",
    "#     # Check if a file for this year's details already exists\n",
    "#     if any(f'StormEvents_details_{year}' in filename for filename in existing_files):\n",
    "#         print(f\"File for details of year {year} already exists. Skipping...\")\n",
    "#     else:\n",
    "#         # If the file doesn't exist, find the original file and modify it\n",
    "#         detail_file = glob.glob(os.path.join(src_dir, f'Storm_event/StormEvents_details-ftp_v1.0_d{year}_c*.csv'))\n",
    "#         for f in detail_file:\n",
    "#             modify_csv(f)\n",
    "#     # Only process 'locations' file if 'details' file already exists and 'locations' does not\n",
    "#     if any(f'StormEvents_details_{year}' in filename for filename in existing_files) and not any(\n",
    "#             f'StormEvents_locations_{year}' in filename for filename in existing_files):\n",
    "#         # If the 'locations' file doesn't exist, find the original file and modify it\n",
    "#         location_file = glob.glob(os.path.join(src_dir, f'Storm_event/StormEvents_locations-ftp_v1.0_d{year}_c*.csv'))\n",
    "#         for f in location_file:\n",
    "#             modify_csv(f)\n",
    "# \n",
    "# # 2) Define range of year files\n",
    "# for year in range(2011, 2021):  # or any other range of years\n",
    "#     # Check if a file for this year's details already exists\n",
    "#     if any(f'StormEvents_details_{year}' in filename for filename in existing_files):\n",
    "#         print(f\"File for details of year {year} already exists. Skipping...\")\n",
    "#     else:\n",
    "#         # If the file doesn't exist, find the original file and modify it\n",
    "#         detail_file = glob.glob(os.path.join(src_dir, f'Storm_event/StormEvents_details-ftp_v1.0_d{year}_c*.csv'))\n",
    "#         for f in detail_file:\n",
    "#             modify_csv(f)\n",
    "#     # Only process 'locations' file if 'details' file already exists and 'locations' does not\n",
    "#     if any(f'StormEvents_details_{year}' in filename for filename in existing_files) and not any(\n",
    "#             f'StormEvents_locations_{year}' in filename for filename in existing_files):\n",
    "#         # If the 'locations' file doesn't exist, find the original file and modify it\n",
    "#         location_file = glob.glob(os.path.join(src_dir, f'Storm_event/StormEvents_locations-ftp_v1.0_d{year}_c*.csv'))\n",
    "#         for f in location_file:\n",
    "#             modify_csv(f)\n",
    "# \n",
    "# # 3) Define all matching CSV files\n",
    "# detail_files = glob.glob(os.path.join(src_dir, 'Storm_event/StormEvents_details-ftp*_d*_c*.csv'))\n",
    "# location_files = glob.glob(os.path.join(src_dir, 'Storm_event/StormEvents_locations-ftp*_d*_c*.csv'))\n",
    "# \n",
    "# # Filter filenames to get base name without year and extension\n",
    "# existing_files = [os.path.splitext(os.path.basename(f))[0].rsplit('_', 1)[0] for f in all_files]\n",
    "# \n",
    "# # Check for each 'details' file\n",
    "# for f in detail_files:\n",
    "#     base_name = os.path.splitext(os.path.basename(f))[0].rsplit('_', 1)[0]\n",
    "#     if base_name not in existing_files:\n",
    "#         modify_csv(f)\n",
    "#         print(f\"{base_name} processed.\")\n",
    "#     else:\n",
    "#     print(f\"{base_name} already exists. Skipping...\")\n",
    "# \n",
    "# # Check for each 'locations' file where 'details' file exists and 'locations' does not\n",
    "# for f in location_files:\n",
    "#     base_name = os.path.splitext(os.path.basename(f))[0].rsplit('_', 1)[0]\n",
    "#     if base_name.replace('locations', 'details') in existing_files and base_name not in existing_files:\n",
    "#         modify_csv(f)\n",
    "#         print(f\"{base_name} processed.\")\n",
    "#     else:\n",
    "#         print(f\"{base_name} already exists or corresponding details file does not exist. Skipping...\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f594298105852323",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO: BROKEN, but I need to sleep. takle tomorrow\n",
    "\n",
    "# # List of Event IDs to be kept even after extent filtering (optional)\n",
    "# event_id_exceptions = []\n",
    "# '''[498859, 508406, 508407, 508408, 508409, 508413, 508414, 508440, 508468, 508476, 508480, 508481,\n",
    "#    508482, 516243, 516244, 516245, 516246, 526836, 526837, 533868, 542919, 543368, 543376, 543645,\n",
    "#    543646, 543667, 627803, 627804, 627806, 630383, 630387, 632580, 633147, 636638, 655474, 659269,\n",
    "#    659272, 659273, 659274, 659292, 660117, 660118, 661872, 661873, 662309, 662310, 663504, 663539,\n",
    "#    663986, 663989, 663991, 754111, 755412, 756557, 756559, 756560, 756562, 756563, 756564, 756569,\n",
    "#    756576, 756578, 756608, 756609, 756611, 756861, 765900, 765905, 772228, 772229, 774573, 774574,\n",
    "#    779898, 779899, 780508, 780781, 787624, 787627, 787630, 792763]\n",
    "#    '''\n",
    "# # Define all matching CSV files\n",
    "# all_files = glob.glob(os.path.join(src_dir, 'Storm_event/StormEvents_*d*_c*.csv'))\n",
    "# details_files = [f for f in all_files if \"details\" in f]\n",
    "# locations_files = [f for f in all_files if \"locations\" in f]\n",
    "# \n",
    "# detail_names = [os.path.splitext(os.path.basename(f))[0] for f in details_files]\n",
    "# location_names = [os.path.splitext(os.path.basename(f))[0] for f in locations_files]\n",
    "# \n",
    "# # # 1) Define year specific files\n",
    "# # years = [2014, 2016, 2018]  # or any other specific years\n",
    "# # for year in years:\n",
    "# #     if (f'StormEvents_details_{year}' in detail_names) and (f'StormEvents_locations_{year}' not in location_names):\n",
    "# #         join_and_save(src_dir, year)\n",
    "# # \n",
    "# # # 2) Define range of year files\n",
    "# # for year in range(2011, 2021):  # or any other range of years\n",
    "# #     if (f'StormEvents_details_{year}' in detail_names) and (f'StormEvents_locations_{year}' not in location_names):\n",
    "# #         join_and_save(src_dir, year)\n",
    "# \n",
    "# # 3) Define all matching CSV files\n",
    "# for detail in detail_names:\n",
    "#     year = detail.split(\"_\")[-1]\n",
    "#     if f'StormEvents_locations_{year}' not in location_names:\n",
    "#         join_and_save(src_dir, year)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb0a26eb2666885a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34687701dcd09892",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "se_gdf_filtered.crs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a89dab1ec147c625",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the storm line data from a shapefile\n",
    "buf_dist_str = '3'\n",
    "stormbuf_gdf = gpd.read_file(os.path.join(src_dir, f'Storm_event/storm_line_{buf_dist_str}_buf.shp'))\n",
    "# Perform the spatial join (intersection)\n",
    "intersect_gdf = gpd.sjoin(sixco_data, stormbuf_gdf, how=\"inner\", op='intersects')\n",
    "intersect_gdf.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a19021a52bf3d51c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(sixco_data.crs)\n",
    "print(stormbuf_gdf.crs)\n",
    "print(intersect_gdf.columns.tolist())\n",
    "print(stormbuf_gdf.columns.tolist())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec9d04a79cea9177",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "events_count_by_year_month_county = intersect_gdf.groupby(['END_YRMO', 'COUNTYFP'])['EVENT_ID'].nunique()\n",
    "\n",
    "print(events_count_by_year_month_county)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e24217bfe38dc93",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7e40d7cb4556705",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Examine Crop Loss Data (under construction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a110898c559012",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the pipe delimited file into a DataFrame without headers\n",
    "col_tbl = pd.read_csv(os.path.join(src_dir, 'crop_loss_COL/colsom_2014.txt'), sep='|', header=None)\n",
    "col_tbl.head()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the Excel file\n",
    "code_header_tbl = pd.read_csv((os.path.join(src_dir, 'crop_loss_COL/dictionary_colsommonth_allyears.csv')), header=None)\n",
    "code_header_tbl.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "859a444655466d17",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Get the headers from the 2nd column (index 1) of the Excel DataFrame\n",
    "tbl_headers = code_header_tbl.iloc[:, 1]\n",
    "\n",
    "# Set the headers in the DataFrame\n",
    "col_tbl.columns = tbl_headers\n",
    "col_tbl.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "285d38a72b17725e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Get unique values\n",
    "unique_states_col_tbl = col_tbl['State Code'].unique()\n",
    "unique_states_sixco_data = sixco_data['STATEFP'].unique()\n",
    "# Display unique values\n",
    "print(unique_states_col_tbl)\n",
    "print(unique_states_sixco_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93adec825e095fad",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Convert 'County Code' values to string and pad with leading zeros\n",
    "col_tbl['County Code'] = col_tbl['County Code'].astype(str).str.zfill(3)\n",
    "col_tbl = col_tbl[col_tbl['State Code'].isin(sixco_data['STATEFP'].astype(int))]\n",
    "# Further filter rows where 'County Code' matches 'COUNTYFP'\n",
    "col_tbl = col_tbl[col_tbl['County Code'].isin(sixco_data['COUNTYFP'].astype(str))]\n",
    "# # Convert 'County Code'  values back to int\n",
    "# col_tbl['County Code'] = col_tbl['County Code'].astype(int)\n",
    "col_tbl.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3fe7873fc599484a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# Get unique values\n",
    "unique_counties = col_tbl['County Code'].unique()\n",
    "unique_states = col_tbl['State Code'].unique()\n",
    "# Display unique\n",
    "print(unique_counties, unique_states)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57808f2cb97c3a39",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Sum 'Indemnity Amount' by 'County Code' and 'Commodity Year Identifier'\n",
    "indemnity_sum = col_tbl.groupby(['Commodity Year Identifier', 'County Code'])['Indemnity Amount'].sum().reset_index()\n",
    "print(indemnity_sum)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55b7578e3f28fb8f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO: Fix the NaN's\n",
    "# Transform `events_count_by_year_month_county` into a DataFrame\n",
    "events_df = events_count_by_year_month_county.reset_index()\n",
    "events_df.columns = ['END_YRMO', 'COUNTYFP', 'EVENT_COUNT']\n",
    "\n",
    "# Extract year from 'END_YRMO'\n",
    "events_df['Year'] = events_df['END_YRMO'].astype(str).str[:4].astype(\n",
    "    int)  # convert to int, or .astype(str) if 'Commodity Year Identifier' is string\n",
    "\n",
    "# Merge with `sixco_data` to get 'NAME'\n",
    "merged_df = pd.merge(events_df, sixco_data[['COUNTYFP', 'NAME']].drop_duplicates(), on='COUNTYFP', how='left')\n",
    "\n",
    "# Convert 'Commodity Year Identifier' in `indemnity_sum` to int (or leave it as is if it's string)\n",
    "indemnity_sum['Commodity Year Identifier'] = indemnity_sum['Commodity Year Identifier'].astype(\n",
    "    int)  # remove this line if 'Commodity Year Identifier' is string\n",
    "# Drop missing values from the 'indemnity_sum' dataset\n",
    "indemnity_sum = indemnity_sum.dropna(subset=['County Code', 'Commodity Year Identifier'])\n",
    "\n",
    "# Merge with `indemnity_sum` to get 'Indemnity Amount'\n",
    "final_df = pd.merge(merged_df, indemnity_sum, left_on=['COUNTYFP', 'Year'],\n",
    "                    right_on=['County Code', 'Commodity Year Identifier'], how='left')\n",
    "print(final_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "402db409268bdf3a",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WindBreaks",
   "language": "python",
   "name": "windbreaks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
